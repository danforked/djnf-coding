{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping\n",
    "\n",
    "Scraping unstructured information from the web into structured data is a common use of Python in the newsroom. In this session, we're going to use a module called [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to do most of the heavy lifting.\n",
    "\n",
    "### Quick overview: HTML\n",
    "\n",
    "To scrape a web page, you need to _sort of_ understand how web pages are made.\n",
    "\n",
    "Web pages are written in HTML. HTML elements are represented (_usually_) by a pair of tags -- an opening tag and a closing tag.\n",
    "\n",
    "A table, for example, starts with `<table>` and ends with `</table>`. The first tag tells the browser: \"Hey! I got a table here! Render it as a table.\" The closing tag (note the forward slash) tells the browser: \"Hey! I'm all done with that table, thanks.\" Inside the table are nested tags representing rows and cells.\n",
    "\n",
    "(There's more to it, but that's probably good for now.)\n",
    "\n",
    "### Inspect the source!\n",
    "\n",
    "If I'm thinking about scraping a page, the first thing I do is look at the HTML code that underpins the page. You can do this right from your browser -- I like to use Chrome but Firefox has some good developer tools, as well. (Maybe IE does too, who knows lol)\n",
    "\n",
    "To \"view source\" in Chrome, you'd hit `Ctrl+U` on a PC and `Cmd+Opt+U` on a Mac. It's also in the menu bar: View -> Developer -> View Page Source.\n",
    "\n",
    "You'll get a page showing you all the HTML code that makes up that page.\n",
    "\n",
    "### BeautifulSoup\n",
    "\n",
    "BeautifulSoup turns HTML into data objects that Python can work with, allowing you to walk up and down the HTML tag \"tree\" and target specific elements on the page.\n",
    "\n",
    "## Warmup\n",
    "\n",
    "Before we start operating on a live page, let's practice on a sample file at `../practice-table.html`. Normally, the first step to scrape a web page is to fetch it -- later, we'll use `requests` for this again -- but for this example let's pretend we've already got a local copy.\n",
    "\n",
    "The Mountain Goats [have a new album out](https://themountaingoats.bandcamp.com/album/goths) (it is good, you should buy it); the HTML we're going to operate on is just a `<table>` showing the track listing.\n",
    "\n",
    "Let's start by importing the `BeautifulSoup` class from our module, which is called `bs4`. We're also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're going to open the file and use the `read()` method to read its contents into memory. Then let's print the contents of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<table id=\"empty-table-to-throw-you-off\"></table>\n",
      "<table class=\"song-table\" id=\"my-cool-table\" style=\"width: 95%;\">\n",
      "  <thead>\n",
      "    <tr>\n",
      "      <th>Track Number</th>\n",
      "      <th>Song Title</th>\n",
      "      <th>Duration</th>\n",
      "      <th>Artist</th>\n",
      "      <th>Album</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <td>1</td>\n",
      "      <td>Rain in Soho</td>\n",
      "      <td>4:47</td>\n",
      "      <td>The Mountain Goats</td>\n",
      "      <td>Goths</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>2</td>\n",
      "      <td>Andrew Eldritch is Moving Back to Leeds</td>\n",
      "      <td>4:19</td>\n",
      "      <td>The Mountain Goats</td>\n",
      "      <td>Goths</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>3</td>\n",
      "      <td>The Grey King and the Silver Flame Attunement</td>\n",
      "      <td>4:55</td>\n",
      "      <td>The Mountain Goats</td>\n",
      "      <td>Goths</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>4</td>\n",
      "      <td>We Do it Different on the West Coast</td>\n",
      "      <td>5:21</td>\n",
      "      <td>The Mountain Goats</td>\n",
      "      <td>Goths</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>5</td>\n",
      "      <td>Unicorn Tolerance</td>\n",
      "      <td>5:25</td>\n",
      "      <td>The Mountain Goats</td>\n",
      "      <td>Goths</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>6</td>\n",
      "      <td>Stench of the Unburied</td>\n",
      "      <td>4:30</td>\n",
      "      <td>The Mountain Goats</td>\n",
      "      <td>Goths</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>7</td>\n",
      "      <td>Wear Black</td>\n",
      "      <td>4:11</td>\n",
      "      <td>The Mountain Goats</td>\n",
      "      <td>Goths</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>8</td>\n",
      "      <td>Paid in Cocaine</td>\n",
      "      <td>3:58</td>\n",
      "      <td>The Mountain Goats</td>\n",
      "      <td>Goths</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>9</td>    \n",
      "      <td>Rage of Travers</td>\n",
      "      <td>5:55</td>\n",
      "      <td>The Mountain Goats</td>\n",
      "      <td>Goths</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>10</td>\n",
      "      <td>Shelved</td>\n",
      "      <td>4:02</td>\n",
      "      <td>The Mountain Goats</td>\n",
      "      <td>Goths</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>11</td>\n",
      "      <td>For the Portuguese Goths Metal Bands</td>\n",
      "      <td>4:10</td>\n",
      "      <td>The Mountain Goats</td>\n",
      "      <td>Goths</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>12</td>\n",
      "      <td>Abandoned Flesh</td>\n",
      "      <td>3:42</td>\n",
      "      <td>The Mountain Goats</td>\n",
      "      <td>Goths</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "with open('../practice-table.html', 'r', encoding='utf-8') as html_file:\n",
    "    html_code = html_file.read()\n",
    "    print(html_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's feed the file contents to a BeautifulSoup object and assign the result to the variable `soup`. You might get an error unless you also pass `'html.parser'` as the second argument. Now print `type(soup)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "with open('../practice-table.html', 'r', encoding='utf-8') as html_file:\n",
    "    html_code = html_file.read()\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    print(type(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. We're locked and loaded. Our string of HTML is now a tree that we can climb through to find the things we want.\n",
    "\n",
    "There are a couple of ways to isolate the table we want using the `find` or `find_all` methods -- by class, by ID, by position on the page, by style. (There are others.) Let's try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table class=\"song-table\" id=\"my-cool-table\" style=\"width: 95%;\">\n",
      "<thead>\n",
      "<tr>\n",
      "<th>Track Number</th>\n",
      "<th>Song Title</th>\n",
      "<th>Duration</th>\n",
      "<th>Artist</th>\n",
      "<th>Album</th>\n",
      "</tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr>\n",
      "<td>1</td>\n",
      "<td>Rain in Soho</td>\n",
      "<td>4:47</td>\n",
      "<td>The Mountain Goats</td>\n",
      "<td>Goths</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>2</td>\n",
      "<td>Andrew Eldritch is Moving Back to Leeds</td>\n",
      "<td>4:19</td>\n",
      "<td>The Mountain Goats</td>\n",
      "<td>Goths</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>3</td>\n",
      "<td>The Grey King and the Silver Flame Attunement</td>\n",
      "<td>4:55</td>\n",
      "<td>The Mountain Goats</td>\n",
      "<td>Goths</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>4</td>\n",
      "<td>We Do it Different on the West Coast</td>\n",
      "<td>5:21</td>\n",
      "<td>The Mountain Goats</td>\n",
      "<td>Goths</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>5</td>\n",
      "<td>Unicorn Tolerance</td>\n",
      "<td>5:25</td>\n",
      "<td>The Mountain Goats</td>\n",
      "<td>Goths</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>6</td>\n",
      "<td>Stench of the Unburied</td>\n",
      "<td>4:30</td>\n",
      "<td>The Mountain Goats</td>\n",
      "<td>Goths</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>7</td>\n",
      "<td>Wear Black</td>\n",
      "<td>4:11</td>\n",
      "<td>The Mountain Goats</td>\n",
      "<td>Goths</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>8</td>\n",
      "<td>Paid in Cocaine</td>\n",
      "<td>3:58</td>\n",
      "<td>The Mountain Goats</td>\n",
      "<td>Goths</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>9</td>\n",
      "<td>Rage of Travers</td>\n",
      "<td>5:55</td>\n",
      "<td>The Mountain Goats</td>\n",
      "<td>Goths</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>10</td>\n",
      "<td>Shelved</td>\n",
      "<td>4:02</td>\n",
      "<td>The Mountain Goats</td>\n",
      "<td>Goths</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>11</td>\n",
      "<td>For the Portuguese Goths Metal Bands</td>\n",
      "<td>4:10</td>\n",
      "<td>The Mountain Goats</td>\n",
      "<td>Goths</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>12</td>\n",
      "<td>Abandoned Flesh</td>\n",
      "<td>3:42</td>\n",
      "<td>The Mountain Goats</td>\n",
      "<td>Goths</td>\n",
      "</tr>\n",
      "</tbody>\n",
      "</table>\n"
     ]
    }
   ],
   "source": [
    "with open('../practice-table.html', 'r', encoding='utf-8') as html_file:\n",
    "    html_code = html_file.read()\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    \n",
    "    # by position on the page\n",
    "    # find_all returns a list of matching elements, and we want the second ([1]) one\n",
    "    # song_table = soup.find_all('table')[1]\n",
    "    \n",
    "    # by class name\n",
    "    # => with `find`, you can pass in a dictionary of element attributes to match on\n",
    "    # song_table = soup.find('table', {'class': 'song-table'})\n",
    "    \n",
    "    # by ID\n",
    "    # song_table = soup.find('table', {'id': 'my-cool-table'})\n",
    "    \n",
    "    # by style\n",
    "    song_table = soup.find('table', {'style': 'width: 95%;'})\n",
    "    \n",
    "    print(song_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've targeted the correct table. Now what if we wanted to print a list of track numbers and song titles? Look at the structure of the table -- a `table` has rows represented by the tag `tr`, and within each row there are cells represented by `td`. The `find_all()` method, you'll recall, returns a _list_. And we know how to iterate over lists: with a `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Rain in Soho\n",
      "2. Andrew Eldritch is Moving Back to Leeds\n",
      "3. The Grey King and the Silver Flame Attunement\n",
      "4. We Do it Different on the West Coast\n",
      "5. Unicorn Tolerance\n",
      "6. Stench of the Unburied\n",
      "7. Wear Black\n",
      "8. Paid in Cocaine\n",
      "9. Rage of Travers\n",
      "10. Shelved\n",
      "11. For the Portuguese Goths Metal Bands\n",
      "12. Abandoned Flesh\n"
     ]
    }
   ],
   "source": [
    "with open('../practice-table.html', 'r', encoding='utf-8') as html_file:\n",
    "    html_code = html_file.read()\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "\n",
    "    song_table = soup.find('table', {'class': 'song-table'})\n",
    "    \n",
    "    table_rows = song_table.find_all('tr')\n",
    "    \n",
    "    # let's skip the header row\n",
    "    # more on list slicing: http://pythoncentral.io/how-to-slice-listsarrays-and-tuples-in-python/\n",
    "    for row in table_rows[1:]:\n",
    "        # get a list of cells in the row\n",
    "        cols = row.find_all('td')\n",
    "        \n",
    "        # the track number is is in the first ([0]) \"column\"\n",
    "        # the `.string` attribute gets the contents of a BeautifulSoup Tag object\n",
    "        track_number = cols[0].string\n",
    "        \n",
    "        # the song title is in the second ([1]) \"column\"\n",
    "        song_title = cols[1].string\n",
    "\n",
    "        print(track_number + '.', song_title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now let's work on a live example.\n",
    "\n",
    "### Scraping etiquette\n",
    "\n",
    "**Rule No. 1: Don't hammer their servers.** If feasible, save a copy of the page locally so you only need to fetch it once as you write your script. For me, at least, scraping involves a lot of \"try this and see if it works,\" which can involve running the same script a dozen (or hundreds) of times as you refine it. If you're requesting multiple pages, pause a for a little bit between requests so you don't overload them.\n",
    "\n",
    "That's good for now. We can go over other rules as problems arise.\n",
    "\n",
    "We are going to scrape [this website](http://www.nrc.gov/reactors/operating/list-power-reactor-units.html) in just Four! Easy! Steps!\n",
    "\n",
    "1. Import libraries\n",
    "2. Grab the contents of the web page\n",
    "3. Parse the contents of the web page and target the data table\n",
    "4. Loop over the table and write the contents to a CSV file\n",
    "\n",
    "### 1. Import libraries\n",
    "\n",
    "We'll need `requests` to fetch the page, `bs4` (the BeautifulSoup class, at least) to parse it, and our old friend `csv` to write out to a new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Grab the contents of the web page\n",
    "\n",
    "`requests` has a method called `get()` that -- you guessed it! -- gets a page. The `text` attribute of the result will give you the string of HTML we need to hand off to BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://www.nrc.gov/reactors/operating/list-power-reactor-units.html'\n",
    "web_page = requests.get(url).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Parse the contents, target the data table\n",
    "\n",
    "Lucky for us, there's only one `<table>` on the page. How do I know this? I viewed the source code and `Ctrl+F`'d for \"&lt;table\". So we can use the `find()` method to get it once we make soup out of that string of HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(web_page, 'html.parser')\n",
    "table = soup.find('table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Loop over the table, write the contents to a CSV\n",
    "\n",
    "Here's where the action is:\n",
    "\n",
    "- Write a function, `cleanString()`, that strips whitespace and garbage characters that throw off Windows\n",
    "- In a `with` block, open a file to write to -- call it `reactors.csv` or something -- in `w` mode\n",
    "- Define your list of headers\n",
    "- Create a writer or DictWriter object\n",
    "- Write headers to the file\n",
    "- Loop over the rows in the table we targeted, extracting the data and writing to the file\n",
    "\n",
    "The first cell in each row is kind of tricky: It has a `<br>` tag to break the line, so you'll need to use the [`contents`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#contents-and-children) attribute to get a tag list, then target the items in that list. You can access the `'href'` attribute of the link in the same way that you'd get a value from a dictionary -- with bracket notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanString(string_in):\n",
    "    \n",
    "    chars_to_replace = ['\\n', '\\r', '\\xa0', '\\xc2', '\\x0A', '\\D0A']\n",
    "\n",
    "    if not string_in:\n",
    "        return ''\n",
    "    else:\n",
    "        for char in chars_to_replace:\n",
    "            string_in = string_in.replace(char, '')\n",
    "            \n",
    "        return string_in.strip()\n",
    "\n",
    "with open('reactors.csv', 'w', encoding='utf-8') as outfile:\n",
    "    \n",
    "    headers = ['name', 'url', 'reactor_id', 'license_no',\n",
    "               'reactor_type', 'location', 'owner_operator', 'nrc_region']\n",
    "\n",
    "    writer = csv.DictWriter(outfile, fieldnames=headers)\n",
    "    \n",
    "    writer.writeheader()\n",
    "    \n",
    "    # loop over the rows in the table\n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        \n",
    "        # get a list of cells in the row\n",
    "        cols = row.find_all('td')\n",
    "\n",
    "        # burst this cell into a list with `contents`\n",
    "        name_cell_contents = cols[0].contents\n",
    "        \n",
    "        # the first item is the name\n",
    "        name = cleanString(name_cell_contents[0].string)\n",
    "        \n",
    "        # the name is wrapped in a link -- let's grab the href\n",
    "        # and prepend the domain\n",
    "        url = 'https://www.nrc.gov' + name_cell_contents[0]['href']\n",
    "        \n",
    "        # the reactor ID is the third thing in the list\n",
    "        reactor_id = cleanString(name_cell_contents[2].string)\n",
    "\n",
    "        license_number = cleanString(cols[1].string)\n",
    "        reactor_type = cleanString(cols[2].string)\n",
    "        location = cleanString(cols[3].string)\n",
    "        owner_operator = cleanString(cols[4].string)\n",
    "        nrc_region = cleanString(cols[5].string)\n",
    "        \n",
    "        # write out to file\n",
    "        writer.writerow({\n",
    "            'name': name,\n",
    "            'url': url,\n",
    "            'reactor_id': reactor_id,\n",
    "            'license_no': license_number,\n",
    "            'reactor_type': reactor_type,\n",
    "            'location': location,\n",
    "            'owner_operator': owner_operator,\n",
    "            'nrc_region': nrc_region\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step: Scraping data from more than one page\n",
    "\n",
    "In theory, we could have just imported that table into Excel. Let's _kick things up a notch_.\n",
    "\n",
    "Problem: We scraped our table, but there is one key piece of information on each reactor's detail page that we want to grab, too: A link to the PDF of its operating license.\n",
    "\n",
    "To get this link, we'll repeat the steps we just took to scrape the overview table -- but we'll tweak it a little bit, adding a function that extracts the link from each detail page.\n",
    "\n",
    "### Extraction function!\n",
    "\n",
    "Let's write an extraction function, `fetchLicensePDF()`, that will take one argument, `detail_html` -- the string of HTML from a reactor's detail page -- and return the extra link we wanna grab.\n",
    "\n",
    "The PDF link is in a list inside another table cell, but we're going to target it using the link's text (\"Plant Operating License\"). It's a relative link, so we're going to prepend the domain before we write out to our file.\n",
    "\n",
    "Some of the pages don't have this link, so we're going to use a [`try/except`](https://docs.python.org/3/tutorial/errors.html) statement to catch errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetchLicensePDF(detail_html):\n",
    "    soup = BeautifulSoup(detail_html, 'html.parser')\n",
    "    \n",
    "    try:\n",
    "        return 'https://www.nrc.gov' + soup.find('a', text='Plant Operating License')['href']\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our functions. We're ready to write out to a new file -- let's call this one 'reactor-detail.csv'. We're going to repeat the same loop we did when we scraped the first table, except this time, we're going to to do more things:\n",
    "\n",
    "1. Grab the detail page link and call the `scrapeDetail()` function on it, then write out the values in the returned dictionary alongside the other data\n",
    "2. Pause for a couple seconds using the `time` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data for Arkansas Nuclear 1\n",
      "Scraped data for Arkansas Nuclear 2\n",
      "Scraped data for Beaver Valley 1\n",
      "Scraped data for Beaver Valley 2\n",
      "Scraped data for Braidwood 1\n",
      "Scraped data for Braidwood 2\n",
      "Scraped data for Browns Ferry 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a3c2d9d764b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Scraped data for '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done! \\o/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "with open('reactors-detail.csv', 'w', encoding='utf-8') as outfile:\n",
    "    \n",
    "    headers = ['name', 'url', 'reactor_id', 'license_no',\n",
    "               'reactor_type', 'location', 'owner_operator',\n",
    "               'nrc_region', 'license_link']\n",
    "\n",
    "    writer = csv.DictWriter(outfile, fieldnames=headers)\n",
    "    \n",
    "    writer.writeheader()\n",
    "    \n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        \n",
    "        cols = row.find_all('td')\n",
    "\n",
    "        name_cell_contents = cols[0].contents\n",
    "        name = cleanString(name_cell_contents[0].string)\n",
    "        url = 'https://www.nrc.gov' + name_cell_contents[0]['href']\n",
    "        reactor_id = cleanString(name_cell_contents[2].string)\n",
    "\n",
    "        license_number = cleanString(cols[1].string)\n",
    "        reactor_type = cleanString(cols[2].string)\n",
    "        location = cleanString(cols[3].string)\n",
    "        owner_operator = cleanString(cols[4].string)\n",
    "        nrc_region = cleanString(cols[5].string)\n",
    "        \n",
    "        # now get the license PDF with these two lines\n",
    "        r = requests.get(url)\n",
    "        license_link = fetchLicensePDF(r.text)            \n",
    "                \n",
    "        writer.writerow({\n",
    "            'name': name,\n",
    "            'url': url,\n",
    "            'reactor_id': reactor_id,\n",
    "            'license_no': license_number,\n",
    "            'reactor_type': reactor_type,\n",
    "            'location': location,\n",
    "            'owner_operator': owner_operator,\n",
    "            'nrc_region': nrc_region,\n",
    "            'license_link': license_link\n",
    "        })\n",
    "                \n",
    "        print('Scraped data for ' + name)\n",
    "        time.sleep(2)\n",
    "        \n",
    "    print('Done! \\o/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
